import torch
from torch import nn
from torch import Tensor
import math

class GELU(nn.Module):
    """
    Gaussian Error Linear Unit (GELU) - функция активации, основанная на свойствах гауссовского распределения.
    
    ## Математическое определение
    
    Исходное определение через кумулятивную функцию распределения (CDF) нормального распределения:
    
    $$\\text{GELU}(x) = x \\cdot \\Phi(x) = x \\cdot \\frac{1}{2} \\left[1 + \\text{erf}\\left(\\frac{x}{\\sqrt{2}}\\right)\\right]$$
    
    где $\\Phi(x)$ - CDF стандартного нормального распределения, $\\text{erf}$ - функция ошибок.
    
    ## Аппроксимация, используемая в реализации
    
    Для эффективного вычисления используется аппроксимация через гиперболический тангенс:
    
    $$\\text{GELU}(x) \\approx 0.5x \\left(1 + \\tanh\\left[\\sqrt{\\frac{2}{\\pi}} \\left(x + 0.044715x^3\\right)\\right]\\right)$$
    
    ## Научное обоснование
    
    ### 1. Вероятностная интерпретация
    GELU можно рассматривать как ожидаемое значение входного сигнала $x$, взвешенное по его вероятности 
    в контексте стохастической регуляризации (как в Dropout). Формально:
    
    $$\\text{GELU}(x) = x \\cdot P(X \\leq x)$$
    
    где $X \\sim \\mathcal{N}(0,1)$.
    
    ### 2. Сравнение с другими функциями активации
    - **ReLU**: $\\max(0, x)$ - жесткое обнуление отрицательных значений
    - **Leaky ReLU**: позволяет небольшой градиент для отрицательных значений
    - **ELU**: плавный переход через ноль, но без вероятностной интерпретации
    
    ### 3. Преимущества GELU
    1. **Дифференцируемость**: функция гладкая везде, что улучшает градиентный спуск
    2. **Нелинейность**: сохраняет сложные паттерны в данных
    3. **Регуляризация**: вероятностная природа обеспечивает естественную регуляризацию
    4. **Устойчивость к затуханию градиента**: лучше сохраняет градиенты по сравнению с сигмоидой/танхом
    
    ### 4. Применение в трансформерах
    В архитектурах типа GPT-2 GELU обеспечивает:
    - Более стабильное обучение глубоких сетей
    - Улучшенную передачу информации между слоями
    - Лучшую обработку контекстной информации в NLP задачах
    
    Эмпирические исследования показывают, что GELU превосходит ReLU и другие активации 
    в задачах обработки естественного языка и компьютерного зрения.
    """
    def __init__(self):
        super().__init__()
        # Создаем тензор для вычисления константы
        self.sqrt_2_over_pi = torch.sqrt(torch.tensor(2.0) / math.pi)
    
    def forward(self, x: Tensor) -> Tensor:
        """
        Прямой проход функции активации GELU.
        
        Вычисляет аппроксимацию:
        $$\\text{GELU}(x) \\approx 0.5x \\left(1 + \\tanh\\left[\\sqrt{\\frac{2}{\\pi}} \\left(x + 0.044715x^3\\right)\\right]\\right)$$
        
        Args:
            x: Входной тензор типа float размером batch_size × seq_len × emb_size
            
        Returns:
            Tensor: Выходной тензор той же размерности, что и входной, после применения GELU
        """
        return 0.5 * x * (1 + torch.tanh(
            self.sqrt_2_over_pi * (x + 0.044715 * torch.pow(x, 3))
        ))